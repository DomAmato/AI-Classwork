import org.junit.Test;

import com.amato.converse.nlp.NLPAnalysis;
import com.amato.converse.nlp.Word2Num;

import edu.mit.jwi.Dictionary;
import edu.mit.jwi.IDictionary;
import edu.mit.jwi.item.IIndexWord;
import edu.mit.jwi.item.ISynset;
import edu.mit.jwi.item.ISynsetID;
import edu.mit.jwi.item.IWord;
import edu.mit.jwi.item.IWordID;
import edu.mit.jwi.item.POS;
import edu.mit.jwi.item.Pointer;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.CoreAnnotations.LemmaAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.time.SUTime;
import edu.stanford.nlp.time.TimeAnnotations;
import edu.stanford.nlp.time.TimeAnnotator;
import edu.stanford.nlp.time.TimeExpression;
import edu.stanford.nlp.time.SUTime.Temporal;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.util.CoreMap;

import static org.junit.Assert.*;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.OutputStreamWriter;
import java.net.MalformedURLException;
import java.net.URL;
import java.time.LocalDate;
import java.time.LocalDateTime;
import java.time.LocalTime;
import java.time.ZoneId;
import java.time.format.DateTimeParseException;
import java.util.ArrayList;
import java.util.Date;
import java.util.Iterator;
import java.util.List;
import java.util.Properties;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.commons.math3.util.Pair;
import org.json.simple.JSONArray;
import org.json.simple.JSONObject;
import org.json.simple.parser.JSONParser;
import org.json.simple.parser.ParseException;

/*
 * This Java source file was auto generated by running 'gradle init --type java-library'
 * by 'DomAA' at '5/24/17 4:23 PM' with Gradle 2.14.1
 *
 * @author DomAA, @date 5/24/17 4:23 PM
 */
public class TestUtility {
	@Test
	public void testCoreNLP(){
		String text = "What is open at 11. five miles away. I want Italian food tomorrow at 8pm. Brunch in Paris today around 12:30. Right now in France. now. Thursday at noon. This weekend around 6.";

		// creates a StanfordCoreNLP object, with POS tagging, lemmatization,
		// NER, parsing, and coreference resolution
		Properties props = new Properties();

		props.put("annotators", "tokenize, ssplit, pos, lemma, ner, parse"); // ,
																				// depparse,
																				// natlog,
																				// openie
		StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
		pipeline.addAnnotator(new TimeAnnotator("sutime", new Properties()));

		System.out.println("NLP Initialized");
		
		Annotation document = new Annotation(text);

		// run all Annotators on this text
		pipeline.annotate(document);

		// these are all the sentences in this document
		// a CoreMap is essentially a Map that uses class objects as keys and
		// has values with custom types
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);

		List<Pair<String, String>> foundEntities = new ArrayList();

		for (CoreMap sentence : sentences) {
			// traversing the words in the current sentence
			// a CoreLabel is a CoreMap with additional token-specific methods
			for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
				// this is the text of the token
				String word = token.get(TextAnnotation.class);
				// this is the POS tag of the token
				String pos = token.get(PartOfSpeechAnnotation.class);
				// this is the NER label of the token
				String ne = token.get(NamedEntityTagAnnotation.class);
				if (ne != "O") {
					foundEntities.add(new Pair(word, ne));
				}
				// The lmma of the word
				String lem = token.get(LemmaAnnotation.class);

				System.out.println("word: " + word + " lemma: " + lem + " pos: " + pos + " ne:" + ne);
			}

			// this is the parse tree of the current sentence
			Tree tree = sentence.get(TreeAnnotation.class);
			System.out.println("parse tree:\n" + tree);

			// // this is the Stanford dependency graph of the current sentence
			// SemanticGraph dependencies =
			// sentence.get(EnhancedPlusPlusDependenciesAnnotation.class);
			// System.out.println("dependency graph:\n" + dependencies);
			//
			// Collection<RelationTriple> triples =
			// sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);
			// // Print the triples
			// for (RelationTriple triple : triples) {
			// System.out.println(triple.confidence + "\t" +
			// triple.subjectLemmaGloss() + "\t"
			// + triple.relationLemmaGloss() + "\t" +
			// triple.objectLemmaGloss());
			// }
			
		}
	}
	
	
	@Test
	public void testOpenNLPTime() {

		String text = "I want Italian food tomorrow at 8. Brunch today around 12:30. Right now. now. Thursday at noon. This weekend around 6.";

		// creates a StanfordCoreNLP object, with POS tagging, lemmatization,
		// NER, parsing, and coreference resolution
		Properties props = new Properties();

		props.put("annotators", "tokenize, ssplit, pos, lemma, ner, parse"); // ,
																				// depparse,
																				// natlog,
																				// openie
		StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
		pipeline.addAnnotator(new TimeAnnotator("sutime", new Properties()));

		System.out.println("NLP Initialized");

		Annotation annotation = new Annotation(text);
		annotation.set(CoreAnnotations.DocDateAnnotation.class, LocalDate.now().toString());
		pipeline.annotate(annotation);
		List<Temporal> possibleTimes = new ArrayList();
		for (CoreMap cm : annotation.get(TimeAnnotations.TimexAnnotations.class)) {
			// List<CoreLabel> tokens =
			// cm.get(CoreAnnotations.TokensAnnotation.class);
			// System.out.println(cm + " [from char offset "
			// +
			// tokens.get(0).get(CoreAnnotations.CharacterOffsetBeginAnnotation.class)
			// + " to "
			// + tokens.get(tokens.size() -
			// 1).get(CoreAnnotations.CharacterOffsetEndAnnotation.class) + ']'
			// + " --> " +
			// cm.get(TimeExpression.Annotation.class).getTemporal());

			possibleTimes.add(cm.get(TimeExpression.Annotation.class).getTemporal());
		}

		long unixTime;
		for (Temporal time : possibleTimes) {
			System.out.print(time + " - " + time.toISOString() + " - " + time.getRange());
			if (time.isApprox() || time.isRef()) {
				System.out.println("Time is a reference, time now is: " + time);
			} else {
				try {
					LocalDate date = LocalDate.parse(time.toISOString().split(Pattern.quote("T"))[0]);
					LocalTime pTime = LocalTime.now();
					if (time.toISOString().contains("T")) {
						pTime = LocalTime.parse(time.toISOString().split(Pattern.quote("T"))[1]);
					}

					unixTime = LocalDateTime.of(date, pTime).atZone(ZoneId.systemDefault()).toEpochSecond();
					System.out.println(
							"Current Epoch: " + System.currentTimeMillis() / 1000L + " Query Epoch: " + unixTime);
				} catch (DateTimeParseException e) {
					String timeString = time.getRange().begin().toISOString();
					LocalDate date = LocalDate
							.parse(timeString.split(Pattern.quote("T"))[0]);
					LocalTime pTime = LocalTime.now();
					if (timeString.contains("T")) {
						pTime = LocalTime.parse(timeString.split(Pattern.quote("T"))[1]);
					}

					unixTime = LocalDateTime.of(date, pTime).atZone(ZoneId.systemDefault()).toEpochSecond();
					System.out.println(
							"Current Epoch: " + System.currentTimeMillis() / 1000L + " Query Epoch: " + unixTime);
				}
			}
		}
	}

	@Test
	public void filterInitalCategories() {
		JSONParser parser = new JSONParser();
		try {
			File jsonFile = new File("categories.json");
			if (jsonFile.exists()) {
				JSONArray jsonOutput = new JSONArray();
				JSONArray jsonObject = (JSONArray) parser.parse(new FileReader(jsonFile));

				for (Object cat : jsonObject) {
					JSONObject jsonCat = (JSONObject) cat;
					if (jsonCat.containsKey("parents")) {
						if (((JSONArray) jsonCat.get("parents")).contains("restaurants")
								|| ((JSONArray) jsonCat.get("parents")).contains("food")) {
							if (jsonCat.containsKey("country_whitelist")) {
								if (((JSONArray) jsonCat.get("country_whitelist")).contains("US")) {
									jsonOutput.add(jsonCat);
								}
							} else if (jsonCat.containsKey("country_blacklist")) {
								if (!((JSONArray) jsonCat.get("country_blacklist")).contains("US")) {
									jsonOutput.add(jsonCat);
								}
							} else {
								jsonOutput.add(jsonCat);
							}
						}
					}
				}

				for (int i = 0; i < 2; i++) {
					for (Object cat : jsonObject) {
						JSONObject jsonCat = (JSONObject) cat;
						if (jsonCat.containsKey("parents")) {
							boolean add = false;
							String matchAlias = "";
							for (Object filtCat : jsonOutput) {
								JSONObject jsonFiltCat = (JSONObject) filtCat;
								if (((JSONArray) jsonCat.get("parents"))
										.contains(((String) jsonFiltCat.get("alias")))) {
									matchAlias = (String) jsonFiltCat.get("alias");
									if (jsonCat.containsKey("country_whitelist")) {
										if (((JSONArray) jsonCat.get("country_whitelist")).contains("US")) {
											add = true;
											break;
										}
									} else if (jsonCat.containsKey("country_blacklist")) {
										if (!((JSONArray) jsonCat.get("country_blacklist")).contains("US")) {
											add = true;
											break;
										}
									} else {
										add = true;
										break;
									}
								}
							}
							if (add) {
								if (!jsonOutput.contains(jsonCat)) {
									System.out.println(
											"Found a matching alias for " + jsonCat.get("alias") + " - " + matchAlias);
									jsonOutput.add(jsonCat);
								}

							}
						}
					}
				}

				File outFile = new File("filtered_categories.json");
				FileOutputStream fOut = new FileOutputStream(outFile);
				OutputStreamWriter myOutWriter = new OutputStreamWriter(fOut);
				jsonOutput.writeJSONString(myOutWriter);
				myOutWriter.close();
				fOut.close();
				assertTrue(true);
			} else {
				System.out.println("Connot Find File");
				assertTrue(false);
			}
		} catch (IOException | ParseException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			assertTrue(false);
		}

	}
	
	@Test
	public void filterFilteredCateogories(){
		Pattern wordPat = Pattern.compile("\\w+");
		JSONParser parser = new JSONParser();
		try {
			File jsonFile = new File("filtered_categories.json");
			if (jsonFile.exists()) {
				JSONArray jsonOutput = new JSONArray();
				JSONArray fileContents = (JSONArray) parser.parse(new FileReader(jsonFile));

				for (Object cat : fileContents) {
					JSONObject jsonCat = (JSONObject) cat;
					
					JSONObject fixedCat = new JSONObject();
					fixedCat.put("alias", jsonCat.get("alias"));
					String text = ((String)jsonCat.get("title"));
					Matcher matcher = wordPat.matcher(text);
					JSONArray wordArray = new JSONArray();
					while(matcher.find()){
						wordArray.add(text.substring(matcher.start(), matcher.end()));
					}
					fixedCat.put("matching_terms", wordArray);
					jsonOutput.add(fixedCat);
				}
				File outFile = new File("final_filtered_categories.json");
				FileOutputStream fOut = new FileOutputStream(outFile);
				OutputStreamWriter myOutWriter = new OutputStreamWriter(fOut);
				jsonOutput.writeJSONString(myOutWriter);
				myOutWriter.close();
				fOut.close();
				assertTrue(true);
			} else {
				System.out.println("Connot Find File");
				assertTrue(false);
			}
		} catch (IOException | ParseException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			assertTrue(false);
		}
	}

	@Test
	public void testNLPTimeFix() {
		String text = "I want Italian food tomorrow at 8. Brunch today around 12:30. Right now. now. Thursday at noon. This weekend around 6.";
		
		List<String> dates = new ArrayList<String>();
		List<String> times = new ArrayList<String>();
		List<String> numbers = new ArrayList<String>();
		List<Pair<String, String>> results = NLPAnalysis.getInstance().analyzeText(text);
		for (Pair<String, String> result : results) {
			if (result.getSecond().equals("DATE")) {
				dates.add(result.getFirst());
			} else if (result.getSecond().equals("TIME")) {
				times.add(result.getFirst());
			} else if (result.getSecond().equals("NUMBER")) {
				numbers.add(result.getFirst());
			}
		}
		
		if(dates.size() > 0 && numbers.size() > 0){
			//the problem here is if the number isnt followed by a colon or an AM/PM than it is not included...
			//the other problem is a date can also have a time... 					
			for(Long time : NLPAnalysis.getInstance().extractTimes(NLPAnalysis.getInstance().replaceNumbersWithTimes(text))){
				System.out.println(new Date(time * 1000));
			}
		}
	}

	// public void getSynonyms ( IDictionary dict ){
	// 2
	// 3 // look up first sense of the word "dog "
	// 4 IIndexWord idxWord = dict . getIndexWord ("dog", POS. NOUN );
	// 5 IWordID wordID = idxWord . getWordIDs ().get (0) ; // 1st meaning
	// 6 IWord word = dict . getWord ( wordID );
	// 7 ISynset synset = word . getSynset ();
	// 8
	// 9 // iterate over words associated with the synset
	// 10 for( IWord w : synset . getWords ())
	// 11 System .out . println (w. getLemma ());
	// 12 }

	// def syn(word, lch_threshold=2.26):
	// for net1 in wn.synsets(word):
	// for net2 in wn.all_synsets():
	// try:
	// lch = net1.lch_similarity(net2)
	// except:
	// continue
	// # The value to compare the LCH to was found empirically.
	// # (The value is very application dependent. Experiment!)
	// if lch >= lch_threshold:
	// yield (net1, net2, lch)

	@Test
	public void testJWI() {
		try {
			// construct the URL to the Wordnet dictionary directory
			URL url = new URL("file", null, new File("dict").getAbsolutePath());

			// construct the dictionary object and open it
			IDictionary dict = new Dictionary(url);
			dict.open();
			IIndexWord idxWord = dict.getIndexWord("yes", POS.NOUN);
			IWordID wordID = idxWord.getWordIDs().get(0); // 1st meaning
			IWord word = dict.getWord(wordID);
			ISynset synset = word.getSynset();

			for (ISynsetID sys : synset.getRelatedSynsets(Pointer.HYPERNYM)) {
				for (ISynsetID sid : dict.getSynset(sys).getRelatedSynsets()) {
					List<IWord> words = dict.getSynset(sid).getWords();
					System.out.print(sid + " {");
					for (Iterator<IWord> i = words.iterator(); i.hasNext();) {
						System.out.print(i.next().getLemma());
						if (i.hasNext())
							System.out.print(", ");
					}
					System.out.println("}");
				}
			}
			for (IWord w : synset.getWords()) {
				System.out.println(w.getLemma());
			}
		} catch (MalformedURLException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}

	@Test
	public void testWord2Vec() {
		// try {
		//
		// INDArray nd = Nd4j.create(new float[]{1,2,3,4},new int[]{2,2});
		// // Gets Path to Text file
		// String filePath = new
		// ClassPathResource("assets/data/raw_sentences.txt").getFile().getAbsolutePath();
		//
		//
		// // Strip white space before and after for each line
		// SentenceIterator iter = new BasicLineIterator(filePath);
		//
		//
		// // Split on white spaces in the line to get words
		// TokenizerFactory t = new DefaultTokenizerFactory();
		//
		// /*
		// CommonPreprocessor will apply the following regex to each token:
		// [\d\.:,"'\(\)\[\]|/?!;]+
		// So, effectively all numbers, punctuation symbols and some special
		// symbols are stripped off.
		// Additionally it forces lower case for all tokens.
		// */
		// t.setTokenPreProcessor(new CommonPreprocessor());
		//
		// Word2Vec vec = new Word2Vec.Builder()
		// .minWordFrequency(5)
		// .iterations(1)
		// .layerSize(100)
		// .seed(42)
		// .windowSize(5)
		// .iterate(iter)
		// .tokenizerFactory(t)
		// .build();
		//
		// System.out.println(vec.similarity("sure", "yes"));
		// } catch (FileNotFoundException e) {
		// // TODO Auto-generated catch block
		// e.printStackTrace();
		// }
	}
}
